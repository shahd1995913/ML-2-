{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOtuRKnXc8SP5GOVvi3b7bb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahd1995913/Machine-Learning/blob/main/5.Twitter_Sentiment_Analysis/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bu-s9nIYFB2"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download twitter-sentiment-analysis-tah\n",
        "! unzip /content/twitter-sentiment-analysis-tah.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "R-xk4OiNYpjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"target\"].value_counts()"
      ],
      "metadata": {
        "id": "erseqJFDYplf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"flag\"].value_counts()"
      ],
      "metadata": {
        "id": "hDzj-_nyYpot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del  df[\"flag\"]\n",
        "del  df[\"id\"]\n",
        "del  df[\"user\"]\n",
        "del  df[\"date\"]"
      ],
      "metadata": {
        "id": "-lPJG2ICaC-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "xvzQBiPNaDBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print useful information about the dataset\n",
        "print(df.info())\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "H2FI0zsDaDEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use regular expressions to replace email addresses, URLs, phone numbers, other numbers\n",
        "text_messages = df['text']\n",
        "\n",
        "# Replace email addresses with 'email'\n",
        "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
        "                                 'emailaddress')\n",
        "\n",
        "# Replace URLs with 'webaddress'\n",
        "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
        "                                  'webaddress')\n",
        "\n",
        "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
        "processed = processed.str.replace(r'£|\\$|@|', '')\n",
        "    \n",
        "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
        "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
        "                                  'phonenumbr')\n",
        "    \n",
        "# Replace numbers with 'numbr'\n",
        "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
      ],
      "metadata": {
        "id": "mCkNlwMEahXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
        "\n",
        "# Replace whitespace between terms with a single space\n",
        "processed = processed.str.replace(r'\\s+', ' ')\n",
        "\n",
        "# Remove leading and trailing whitespace\n",
        "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
      ],
      "metadata": {
        "id": "oYRRIDUkahaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed"
      ],
      "metadata": {
        "id": "d_9gbvzCahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change words to lower case - Hello, HELLO, hello are all the same word\n",
        "processed = processed.str.lower()\n",
        "print(processed)"
      ],
      "metadata": {
        "id": "fUFJ7HI7ahf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# remove stop words from text messages\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processed = processed.apply(lambda x: \" \".join(x.lower() for x in str(x).split() \\\n",
        "                                    if x not in stop_words))\n",
        "print(processed)\n"
      ],
      "metadata": {
        "id": "fgvBfZteahjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove word stems using a Porter stemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "processed = processed.apply(lambda x: ' '.join(\n",
        "    ps.stem(term) for term in x.split()))\n",
        "print(processed)"
      ],
      "metadata": {
        "id": "LRXd0UQFbyJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# create bag-of-words\n",
        "all_words = []\n",
        "\n",
        "for message in processed:\n",
        "    words = word_tokenize(message)\n",
        "    for w in words:\n",
        "        all_words.append(w)\n",
        "        \n",
        "all_words = nltk.FreqDist(all_words)"
      ],
      "metadata": {
        "id": "dhtRHwoUbyMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the total number of words and the 15 most common words\n",
        "print('Number of words: {}'.format(len(all_words)))\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))"
      ],
      "metadata": {
        "id": "VHw_HI4obyPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the 1500 most common words as features\n",
        "word_features = list(all_words.keys())[:1500]"
      ],
      "metadata": {
        "id": "HYb7bQZpbyRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The find_features function will determine which of the 1500 word features are contained in the review\n",
        "def find_features(message):\n",
        "    words = word_tokenize(message)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Lets see an example!\n",
        "features = find_features(processed[0])\n",
        "for key, value in features.items():\n",
        "    if value == True:\n",
        "        print(key)"
      ],
      "metadata": {
        "id": "EMbwRmOXbyUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets do it for all the messages\n",
        "messages = list(zip(processed, Y))\n",
        "\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed = seed\n",
        "np.random.shuffle(messages)\n",
        "\n",
        "# call find_features function for each SMS message\n",
        "featuresets = [(find_features(text), label) for (text, label) in messages]"
      ],
      "metadata": {
        "id": "I4G7ocGMbyXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9NwVhWymcOKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vqgQrD29cONG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BJimmQ0gcOQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}